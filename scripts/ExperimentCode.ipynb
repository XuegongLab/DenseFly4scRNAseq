{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import os\n",
    "from collections import OrderedDict as odict\n",
    "from functools import reduce\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix,cohen_kappa_score\n",
    "from bokeh.plotting import figure,output_file,output_notebook,show\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations of SimHash, FlyHash, and DenseFly\n",
    "\n",
    "The implementation of these LSH algorithm are adopted from https://github.com/dataplayer12/Fly-LSH\n",
    "\n",
    "Our code follows MIT License\n",
    "\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2017 Jaiyam Sharma\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimHash implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSH(object):\n",
    "    def __init__(self,data,label,hash_length,nnn):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: the projection numbers\n",
    "        nnn: n nearest Neighbors\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True) #preprocessing\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0] #get group numbers\n",
    "        self.weights=np.random.random((data.shape[1],hash_length))#save projection directions\n",
    "        self.hashes=(self.data@self.weights)>0  #get hash\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        self.create_bins()\n",
    "    \n",
    "    #query one cell in the reference data and retrun the nnn Nearest Neighbors\n",
    "    def query(self,qidx,nnn,not_olap=False):\n",
    "        L1_distances=np.sum(np.abs(self.hashes[qidx,:]^self.hashes),axis=1)\n",
    "        nnn=min(self.hashes.shape[0],nnn)#make nnn reansonable(not too big)\n",
    "        if not_olap:\n",
    "            no_overlaps=np.sum(L1_distances==self.maxl1distance)\n",
    "            return no_overlaps\n",
    "        NNs=L1_distances.argsort()\n",
    "        NNs=NNs[(NNs != qidx)][:nnn]\n",
    "        return NNs\n",
    "    \n",
    "    #creat bins for multiprobe\n",
    "    def create_bins(self):\n",
    "        if hasattr(self,'bins'):\n",
    "            return \n",
    "        self.bins=np.unique(self.hashes,axis=0)\n",
    "        self.num_bins=self.bins.shape[0]\n",
    "        assignment=np.zeros(self.hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.bins):\n",
    "            assignment[(self.hashes==_bin).all(axis=1)]=idx\n",
    "        self.binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.bins.shape[0])}\n",
    "        self.pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "    \n",
    "    #multiprobe query, query one cell in the reference dataset and return nnn NNs in the search_radius (if there are)\n",
    "    def query_bins(self,qidx,search_radius=1,order=True):\n",
    "        if not hasattr(self,'bins'):\n",
    "            raise ValueError('Bins for model not created')\n",
    "        query_bin=self.bins[self.pointstobins[qidx]]\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.bins).sum(axis=1)<=search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        all_points=all_points[1:self.nnn+1]\n",
    "        return all_points\n",
    "    \n",
    "    # query each cell in the indices and return the confusion matrix\n",
    "    def get_confusion_matrix(self,indices):\n",
    "        cm = np.zeros((self.group_counts,self.group_counts))\n",
    "        for idx in indices:\n",
    "            pre_label = self.label[self.query(idx,self.nnn)]\n",
    "            true_label = np.array([self.label[idx]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(self.label))\n",
    "        return cm\n",
    "    \n",
    "    # compute conhen kappa score of a confusion matrix\n",
    "    def compute_CKS(self,matrix):\n",
    "        po = matrix.trace()/np.sum(matrix)\n",
    "        pe = sum(np.sum(matrix,axis=0)*np.sum(matrix,axis=1))/np.sum(matrix)/np.sum(matrix)\n",
    "        return (po-pe)/(1-pe)\n",
    "    \n",
    "    # five-fold cross validation\n",
    "    def test_performance(self):\n",
    "        if not hasattr(self,'bins'):\n",
    "            self.create_bins()\n",
    "        CKS = 0\n",
    "        for step in range(5):\n",
    "            query_indices = np.random.choice(self.data.shape[0],self.data.shape[0]//5)\n",
    "            CM = self.get_confusion_matrix(query_indices)\n",
    "            CKS += self.compute_CKS(CM)/5\n",
    "        return CKS    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlyHash implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flylsh(LSH):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: scalar\n",
    "        nnn: n nearest neighbors\n",
    "        sampling_ratio: fraction of input dims to sample from when producing a hash\n",
    "        embedding_size: dimensionality of projection space, m\n",
    "        Note that in Flylsh, the hash length and embedding_size are NOT the same\n",
    "        whereas in usual LSH they are\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True) #preprocessing\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0]\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        # get high dimension hashes\n",
    "        num_projections=int(sampling_ratio*data.shape[1])\n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        yindices=np.arange(weights.shape[1])[None,:]\n",
    "        xindices=weights.argsort(axis=0)[-num_projections:,:]\n",
    "        self.weights=np.zeros_like(weights,dtype=np.bool)\n",
    "        self.weights[xindices,yindices]= True#sparse projection vectors\n",
    "        \n",
    "        all_activations=(self.data@self.weights)\n",
    "        xindices=np.arange(data.shape[0])[:,None]\n",
    "        yindices=all_activations.argsort(axis=1)[:,-hash_length:]\n",
    "        self.hashes=np.zeros_like(all_activations,dtype=np.bool)\n",
    "        self.hashes[xindices,yindices]=True #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0 # get low dimension hashes\n",
    "        \n",
    "        self.create_lowd_bins()\n",
    "    \n",
    "    #unused \n",
    "    def create_highd_bins(self,d,rounds=1):\n",
    "        \"\"\"\n",
    "        This function implements a relaxed binning for FlyLSH\n",
    "        This is only one of the many possible implementations for such a scheme\n",
    "        d: the number of bits to match between hashes for putting them in the same bin\n",
    "        \"\"\"\n",
    "        self.highd_bins=self.hashes[0:1,:] #initialize hashes to first point\n",
    "        self.highd_binstopoints,self.highd_pointstobins={},{i:[] for i in range(self.hashes.shape[0])}\n",
    "        for round in range(rounds):\n",
    "            for hash_idx,this_hash in enumerate(self.hashes):\n",
    "                overlap=(self.maxl1distance-((this_hash[None,:]^self.highd_bins).sum(axis=1)))>=2*d\n",
    "                #print(overlap.shape)\n",
    "                if overlap.any():\n",
    "                    indices=np.flatnonzero(overlap)\n",
    "                    #indices=indices.tolist()\n",
    "                    #print(indices)\n",
    "                    self.highd_pointstobins[hash_idx].extend(indices)\n",
    "                    for idx in indices:\n",
    "                        if idx not in self.highd_binstopoints:\n",
    "                            #print(indices,idx)\n",
    "                            self.highd_binstopoints[idx]=[]\n",
    "                        self.highd_binstopoints[idx].append(hash_idx)\n",
    "                else:\n",
    "                    self.highd_bins=np.append(self.highd_bins,this_hash[None,:],axis=0)\n",
    "                    bin_idx=self.highd_bins.shape[0]-1\n",
    "                    self.highd_pointstobins[hash_idx].append(bin_idx)\n",
    "                    self.highd_binstopoints[bin_idx]=[hash_idx]\n",
    "    \n",
    "    # create bins for multiprobe \n",
    "    def create_lowd_bins(self):\n",
    "        start=time.time()\n",
    "        self.lowd_bins=np.unique(self.lowd_hashes,axis=0)\n",
    "        #self.num_bins=self.bins.shape[0]\n",
    "\n",
    "        assignment=np.zeros(self.lowd_hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.lowd_bins):\n",
    "            assignment[(self.lowd_hashes==_bin).all(axis=1)]=idx\n",
    "        self.lowd_binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.lowd_bins.shape[0])}\n",
    "        self.lowd_pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "        self.timetoindex=time.time()-start\n",
    "    \n",
    "    # multiprobe query\n",
    "    def query_lowd_bins(self,qidx,search_radius=1,order=True):\n",
    "        if not hasattr(self,'lowd_bins'):\n",
    "            raise ValueError('low dimensional bins for model not created')\n",
    "        query_bin=self.lowd_bins[self.lowd_pointstobins[qidx]]\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=search_radius)\n",
    "        #valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=2*search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.lowd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        all_points=all_points[1:self.nnn+1]\n",
    "        return all_points\n",
    "    \n",
    "    #unused\n",
    "    def query_highd_bins(self,qidx,order=False):\n",
    "        if not hasattr(self,'highd_bins'):\n",
    "            raise ValueError('high dimensional bins for model not created')\n",
    "        valid_bins=self.highd_pointstobins[qidx]\n",
    "        all_points=reduce(np.union1d,np.array([self.highd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        return all_points\n",
    "    \n",
    "    #get confusion matrix your choose the radius to replace 40 or just use the query function without multiprobe\n",
    "    def get_confusion_matrix(self,indices):\n",
    "        cm = np.zeros((self.group_counts,self.group_counts))\n",
    "        for idx in indices:\n",
    "            # multiprobe version\n",
    "            #pre_label = self.label[self.query_lowd_bins(idx,40)]\n",
    "            #if pre_label.shape[0] ==0:\n",
    "            #    pre_label = self.label[self.query_lowd_bins(idx,40*2)]\n",
    "            \n",
    "            # no-multiprobe version\n",
    "            pre_label = self.label[self.query(idx,self.nnn)]\n",
    "            \n",
    "            true_label = np.array([self.label[idx]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(self.label))\n",
    "        return cm\n",
    "    \n",
    "    def test_performance(self):\n",
    "        if not hasattr(self,'bins'):\n",
    "            self.create_lowd_bins()\n",
    "        CKS = 0\n",
    "        for step in range(5):\n",
    "            query_indices = np.random.choice(self.data.shape[0],self.data.shape[0]//5)\n",
    "            CM = self.get_confusion_matrix(query_indices)\n",
    "            CKS += self.compute_CKS(CM)/5\n",
    "        return CKS  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseFly implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denseflylsh(flylsh):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: scalar\n",
    "        nnn: n nearest neighbors\n",
    "        sampling_ratio: fraction of input dims to sample from when producing a hash\n",
    "        embedding_size: dimensionality of projection space, m\n",
    "        Note that in Flylsh, the hash length and embedding_size are NOT the same\n",
    "        whereas in usual LSH they are\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True)\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0]\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        self.weights=(weights>1-sampling_ratio) #sparse projection vectors\n",
    "        all_activations=(self.data@self.weights)\n",
    "        threshold=0\n",
    "        self.hashes=(all_activations>=threshold) #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        \n",
    "        self.create_lowd_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Self-mapping\n",
    "\n",
    "mapping cells within one datset: use one cell as a query and use remaining cells as the reference. \n",
    "\n",
    "Please run `Rscript data_generator\\SIM_I.R` to obtain the simulation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data and label\n",
    "path = \"SelfMapping_Data.txt\"    \n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "for hash_length in [64,128,256,512,1024]:\n",
    "    print('hash length:',hash_length)\n",
    "    # set parameters\n",
    "    nnn=1\n",
    "    # construct models\n",
    "    lshmodel = LSH(inputs_,labels_,hash_length,nnn)\n",
    "    flymodel = flylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    denseflymodel = denseflylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    # test performance\n",
    "    print(lshmodel.test_performance())\n",
    "    print(flymodel.test_performance())\n",
    "    print(denseflymodel.test_performance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2: Cross-batch mapping\n",
    "\n",
    "mapping cells of the same type across two batches: use cells from one batch  and use cells of the other batch as the reference. \n",
    "\n",
    "Please run `Rscript data_generator\\SIM_II.R` to obtain the simulation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batcheffect(model_1,model_2):\n",
    "    # query cells from batch 1 with batch 2 data as the reference\n",
    "    cm = 0\n",
    "    for i in range(5):\n",
    "        query_indices = np.random.choice(model_1.data.shape[0],model_1.data.shape[0]//5)\n",
    "        for index in query_indices:\n",
    "            NNs = model_2.query(model_1.data[index],model_2.nnn)\n",
    "            pre_label= model_2.label[NNs]\n",
    "            true_label = np.array([model_1.label[index]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(model_2.label))\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data and divide matrix into two batches and their labels\n",
    "path = \"Batch_Data.txt\"    \n",
    "data=pd.read_table(path,sep=' ')\n",
    "index_b1=data[\"Batch\"]==\"Batch1\"\n",
    "index_b2=data[\"Batch\"]==\"Batch2\"\n",
    "data_1 = np.array(data[index_b1].iloc[:,:-2])\n",
    "label_1 = np.array(data[index_b1].iloc[:,-1])\n",
    "data_2 = np.array(data[index_b2].iloc[:,:-2])\n",
    "label_2 = np.array(data[index_b2].iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting parameters\n",
    "hash_length=64 # hash length can be selected from [64,128,256,512,1024]\n",
    "nnn=10\n",
    "#construct DenseFly models\n",
    "print('DenseFly')\n",
    "densemodel_1=denseflylsh(data_1,label_1,hash_length,nnn,0.1,20*hash_length)\n",
    "densemodel_2=denseflylsh(data_2,label_2,hash_length,nnn,0.1,20*hash_length)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(densemodel_1,densemodel_2)\n",
    "cm_2to1 = test_batcheffect(densemodel_2,densemodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)\n",
    "\n",
    "#construct FlyHash models\n",
    "print('FlyHash')\n",
    "flymodel_1=flylsh(data_1,label_1,hash_length,nnn,0.1,20*hash_length)\n",
    "flymodel_2=flylsh(data_2,label_2,hash_length,nnn,0.1,20*hash_length)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(flymodel_1,flymodel_2)\n",
    "cm_2to1 = test_batcheffect(flymodel_2,flymodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)\n",
    "\n",
    "#construct SimHash models\n",
    "print('SimHash')\n",
    "lshmodel_1=LSH(data_1,label_1,hash_length,nnn)\n",
    "lshmodel_2=LSH(data_2,label_2,hash_length,nnn)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(lshmodel_1,lshmodel_2)\n",
    "cm_2to1 = test_batcheffect(lshmodel_2,lshmodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3: Dropout events\n",
    "\n",
    "mapping cells within one datset: Try different dropout rate. \n",
    "\n",
    "Please run `Rscript data_generator\\SIM_III.R` to obtain the simulation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test robust of dropout\n",
    "\n",
    "# get data and label\n",
    "path = \"Dropout_X_Data.txt\"  #!!!!####### please change X to 0,1,2,3,4,5 #########!!!!#\n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "for i in [32,64,128,256]:\n",
    "    print(i)\n",
    "    # set parameters\n",
    "    hash_length=i\n",
    "    nnn=1\n",
    "    # construct models\n",
    "    lshmodel = LSH(inputs_,labels_,hash_length,nnn)\n",
    "    flymodel = flylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    denseflymodel = denseflylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    # test performance\n",
    "    print(lshmodel.test_performance())\n",
    "    print(flymodel.test_performance())\n",
    "    print(denseflymodel.test_performance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the relationship between cell numbers and query time\n",
    "\n",
    "# get data and label\n",
    "path = \"SelfMapping_Data.txt\" \n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "#chooses subset\n",
    "indexs=np.random.choice(range(2000),xxx)   ### please chaneg xxx to the sample number you want , such as 200\n",
    "inputs=inputs_[indexs,:]\n",
    "labels=labels_[indexs]\n",
    "\n",
    "#set parameters\n",
    "hash_length=128\n",
    "nnn=1\n",
    "radius=40\n",
    "\n",
    "# construct models\n",
    "lshmodel = LSH(inputs,labels,hash_length,nnn)\n",
    "flymodel = flylsh(inputs,labels,hash_length,nnn,0.1,20*hash_length)\n",
    "denseflymodel = denseflylsh(inputs,labels,hash_length,nnn,0.1,20*hash_length)\n",
    "\n",
    "#test time\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    lshmodel.query_bins(i,nnn,radius)\n",
    "print('SimHash:',time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    flymodel.query_lowd_bins(i,nnn,radius)\n",
    "print('FlyHash:',time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    denseflymodel.query_lowd_bins(i,nnn,radius)\n",
    "print('DenseFly:',time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
