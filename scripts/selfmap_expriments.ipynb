{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import os\n",
    "from collections import OrderedDict as odict\n",
    "from functools import reduce\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix,cohen_kappa_score\n",
    "from bokeh.plotting import figure,output_file,output_notebook,show\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SimHash\n",
    "class LSH(object):\n",
    "    def __init__(self,data,label,hash_length,nnn):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: the projection numbers\n",
    "        nnn: n nearest Neighbors\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True) #preprocessing\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0] #get group numbers\n",
    "        self.weights=np.random.random((data.shape[1],hash_length))#save projection directions\n",
    "        self.hashes=(self.data@self.weights)>0  #get hash\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        self.create_bins()\n",
    "    \n",
    "    #query one cell in the reference data and retrun the nnn Nearest Neighbors\n",
    "    def query(self,qidx,nnn,not_olap=False):\n",
    "        L1_distances=np.sum(np.abs(self.hashes[qidx,:]^self.hashes),axis=1)\n",
    "        nnn=min(self.hashes.shape[0],nnn)#make nnn reansonable(not too big)\n",
    "        if not_olap:\n",
    "            no_overlaps=np.sum(L1_distances==self.maxl1distance)\n",
    "            return no_overlaps\n",
    "        NNs=L1_distances.argsort()\n",
    "        NNs=NNs[(NNs != qidx)][:nnn]\n",
    "        return NNs\n",
    "    \n",
    "    #creat bins for multiprobe\n",
    "    def create_bins(self):\n",
    "        if hasattr(self,'bins'):\n",
    "            return \n",
    "        self.bins=np.unique(self.hashes,axis=0)\n",
    "        self.num_bins=self.bins.shape[0]\n",
    "        assignment=np.zeros(self.hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.bins):\n",
    "            assignment[(self.hashes==_bin).all(axis=1)]=idx\n",
    "        self.binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.bins.shape[0])}\n",
    "        self.pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "    \n",
    "    #multiprobe query, query one cell in the reference dataset and return nnn NNs in the search_radius (if there are)\n",
    "    def query_bins(self,qidx,search_radius=1,order=True):\n",
    "        if not hasattr(self,'bins'):\n",
    "            raise ValueError('Bins for model not created')\n",
    "        query_bin=self.bins[self.pointstobins[qidx]]\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.bins).sum(axis=1)<=search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        all_points=all_points[1:self.nnn+1]\n",
    "        return all_points\n",
    "    \n",
    "    # query each cell in the indices and return the confusion matrix\n",
    "    def get_confusion_matrix(self,indices):\n",
    "        cm = np.zeros((self.group_counts,self.group_counts))\n",
    "        for idx in indices:\n",
    "            pre_label = self.label[self.query(idx,self.nnn)]\n",
    "            true_label = np.array([self.label[idx]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(self.label))\n",
    "        return cm\n",
    "    \n",
    "    # compute conhen kappa score of a confusion matrix\n",
    "    def compute_CKS(self,matrix):\n",
    "        po = matrix.trace()/np.sum(matrix)\n",
    "        pe = sum(np.sum(matrix,axis=0)*np.sum(matrix,axis=1))/np.sum(matrix)/np.sum(matrix)\n",
    "        return (po-pe)/(1-pe)\n",
    "    \n",
    "    # five-fold cross validation\n",
    "    def test_performance(self):\n",
    "        if not hasattr(self,'bins'):\n",
    "            self.create_bins()\n",
    "        CKS = 0\n",
    "        for step in range(5):\n",
    "            query_indices = np.random.choice(self.data.shape[0],self.data.shape[0]//5)\n",
    "            CM = self.get_confusion_matrix(query_indices)\n",
    "            CKS += self.compute_CKS(CM)/5\n",
    "        return CKS    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FlyHash\n",
    "class flylsh(LSH):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: scalar\n",
    "        nnn: n nearest neighbors\n",
    "        sampling_ratio: fraction of input dims to sample from when producing a hash\n",
    "        embedding_size: dimensionality of projection space, m\n",
    "        Note that in Flylsh, the hash length and embedding_size are NOT the same\n",
    "        whereas in usual LSH they are\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True) #preprocessing\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0]\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        # get high dimension hashes\n",
    "        num_projections=int(sampling_ratio*data.shape[1])\n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        yindices=np.arange(weights.shape[1])[None,:]\n",
    "        xindices=weights.argsort(axis=0)[-num_projections:,:]\n",
    "        self.weights=np.zeros_like(weights,dtype=np.bool)\n",
    "        self.weights[xindices,yindices]= True#sparse projection vectors\n",
    "        \n",
    "        all_activations=(self.data@self.weights)\n",
    "        xindices=np.arange(data.shape[0])[:,None]\n",
    "        yindices=all_activations.argsort(axis=1)[:,-hash_length:]\n",
    "        self.hashes=np.zeros_like(all_activations,dtype=np.bool)\n",
    "        self.hashes[xindices,yindices]=True #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0 # get low dimension hashes\n",
    "        \n",
    "        self.create_lowd_bins()\n",
    "    \n",
    "    #unused \n",
    "    def create_highd_bins(self,d,rounds=1):\n",
    "        \"\"\"\n",
    "        This function implements a relaxed binning for FlyLSH\n",
    "        This is only one of the many possible implementations for such a scheme\n",
    "        d: the number of bits to match between hashes for putting them in the same bin\n",
    "        \"\"\"\n",
    "        self.highd_bins=self.hashes[0:1,:] #initialize hashes to first point\n",
    "        self.highd_binstopoints,self.highd_pointstobins={},{i:[] for i in range(self.hashes.shape[0])}\n",
    "        for round in range(rounds):\n",
    "            for hash_idx,this_hash in enumerate(self.hashes):\n",
    "                overlap=(self.maxl1distance-((this_hash[None,:]^self.highd_bins).sum(axis=1)))>=2*d\n",
    "                #print(overlap.shape)\n",
    "                if overlap.any():\n",
    "                    indices=np.flatnonzero(overlap)\n",
    "                    #indices=indices.tolist()\n",
    "                    #print(indices)\n",
    "                    self.highd_pointstobins[hash_idx].extend(indices)\n",
    "                    for idx in indices:\n",
    "                        if idx not in self.highd_binstopoints:\n",
    "                            #print(indices,idx)\n",
    "                            self.highd_binstopoints[idx]=[]\n",
    "                        self.highd_binstopoints[idx].append(hash_idx)\n",
    "                else:\n",
    "                    self.highd_bins=np.append(self.highd_bins,this_hash[None,:],axis=0)\n",
    "                    bin_idx=self.highd_bins.shape[0]-1\n",
    "                    self.highd_pointstobins[hash_idx].append(bin_idx)\n",
    "                    self.highd_binstopoints[bin_idx]=[hash_idx]\n",
    "    \n",
    "    # create bins for multiprobe \n",
    "    def create_lowd_bins(self):\n",
    "        start=time.time()\n",
    "        self.lowd_bins=np.unique(self.lowd_hashes,axis=0)\n",
    "        #self.num_bins=self.bins.shape[0]\n",
    "\n",
    "        assignment=np.zeros(self.lowd_hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.lowd_bins):\n",
    "            assignment[(self.lowd_hashes==_bin).all(axis=1)]=idx\n",
    "        self.lowd_binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.lowd_bins.shape[0])}\n",
    "        self.lowd_pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "        self.timetoindex=time.time()-start\n",
    "    \n",
    "    # multiprobe query\n",
    "    def query_lowd_bins(self,qidx,search_radius=1,order=True):\n",
    "        if not hasattr(self,'lowd_bins'):\n",
    "            raise ValueError('low dimensional bins for model not created')\n",
    "        query_bin=self.lowd_bins[self.lowd_pointstobins[qidx]]\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=search_radius)\n",
    "        #valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=2*search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.lowd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        all_points=all_points[1:self.nnn+1]\n",
    "        return all_points\n",
    "    \n",
    "    #unused\n",
    "    def query_highd_bins(self,qidx,order=False):\n",
    "        if not hasattr(self,'highd_bins'):\n",
    "            raise ValueError('high dimensional bins for model not created')\n",
    "        valid_bins=self.highd_pointstobins[qidx]\n",
    "        all_points=reduce(np.union1d,np.array([self.highd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        return all_points\n",
    "    \n",
    "    #get confusion matrix your choose the radius to replace 40 or just use the query function without multiprobe\n",
    "    def get_confusion_matrix(self,indices):\n",
    "        cm = np.zeros((self.group_counts,self.group_counts))\n",
    "        for idx in indices:\n",
    "            # multiprobe version\n",
    "            #pre_label = self.label[self.query_lowd_bins(idx,40)]\n",
    "            #if pre_label.shape[0] ==0:\n",
    "            #    pre_label = self.label[self.query_lowd_bins(idx,40*2)]\n",
    "            \n",
    "            # no-multiprobe version\n",
    "            pre_label = self.label[self.query(idx,self.nnn)]\n",
    "            \n",
    "            true_label = np.array([self.label[idx]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(self.label))\n",
    "        return cm\n",
    "    \n",
    "    def test_performance(self):\n",
    "        if not hasattr(self,'bins'):\n",
    "            self.create_lowd_bins()\n",
    "        CKS = 0\n",
    "        for step in range(5):\n",
    "            query_indices = np.random.choice(self.data.shape[0],self.data.shape[0]//5)\n",
    "            CM = self.get_confusion_matrix(query_indices)\n",
    "            CKS += self.compute_CKS(CM)/5\n",
    "        return CKS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DenseFly\n",
    "class denseflylsh(flylsh):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: scalar\n",
    "        nnn: n nearest neighbors\n",
    "        sampling_ratio: fraction of input dims to sample from when producing a hash\n",
    "        embedding_size: dimensionality of projection space, m\n",
    "        Note that in Flylsh, the hash length and embedding_size are NOT the same\n",
    "        whereas in usual LSH they are\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True)\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0]\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        self.weights=(weights>1-sampling_ratio) #sparse projection vectors\n",
    "        all_activations=(self.data@self.weights)\n",
    "        threshold=0\n",
    "        self.hashes=(all_activations>=threshold) #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        \n",
    "        self.create_lowd_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test performance\n",
    "\n",
    "# get data and label\n",
    "path = \"../data/Toy_Data/Toy_Data.txt\"    \n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "for hash_length in [64,128,256,512,1024]:\n",
    "    print('hash length:',hash_length)\n",
    "    # set parameters\n",
    "    nnn=1\n",
    "    # construct models\n",
    "    lshmodel = LSH(inputs_,labels_,hash_length,nnn)\n",
    "    flymodel = flylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    denseflymodel = denseflylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    # test performance\n",
    "    print(lshmodel.test_performance())\n",
    "    print(flymodel.test_performance())\n",
    "    print(denseflymodel.test_performance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test robust of dropout\n",
    "\n",
    "# get data and label\n",
    "path = \"../data/Dropout_Data/Dropout_x_Data.txt\"  ### please change x to 0,1,2,3,4,5\n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "for i in [32,64,128,256]:\n",
    "    print(i)\n",
    "    # set parameters\n",
    "    hash_length=i\n",
    "    nnn=1\n",
    "    # construct models\n",
    "    lshmodel = LSH(inputs_,labels_,hash_length,nnn)\n",
    "    flymodel = flylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    denseflymodel = denseflylsh(inputs_,labels_,hash_length,nnn,0.1,20*hash_length)\n",
    "    # test performance\n",
    "    print(lshmodel.test_performance())\n",
    "    print(flymodel.test_performance())\n",
    "    print(denseflymodel.test_performance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the relationship between cell numbers and query time\n",
    "\n",
    "# get data and label\n",
    "path = \"../data/Toy_Data/Toy_Data.txt\" \n",
    "data = pd.read_table(path,sep=' ')\n",
    "labels_=np.array(data.iloc[:,-1])\n",
    "inputs_=np.array(data.iloc[:,:-1])\n",
    "\n",
    "#chooses subset\n",
    "indexs=np.random.choice(range(2000),xxx)   ### please chaneg xxx to the sample number you want , such as 200\n",
    "inputs=inputs_[indexs,:]\n",
    "labels=labels_[indexs]\n",
    "\n",
    "#set parameters\n",
    "hash_length=128\n",
    "nnn=1\n",
    "radius=40\n",
    "\n",
    "# construct models\n",
    "lshmodel = LSH(inputs,labels,hash_length,nnn)\n",
    "flymodel = flylsh(inputs,labels,hash_length,nnn,0.1,20*hash_length)\n",
    "denseflymodel = denseflylsh(inputs,labels,hash_length,nnn,0.1,20*hash_length)\n",
    "\n",
    "#test time\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    lshmodel.query_bins(i,nnn,radius)\n",
    "print('SimHash:',time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    flymodel.query_lowd_bins(i,nnn,radius)\n",
    "print('FlyHash:',time.time()-start)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    denseflymodel.query_lowd_bins(i,nnn,radius)\n",
    "print('DenseFly:',time.time()-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (densefly)",
   "language": "python",
   "name": "densefly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
