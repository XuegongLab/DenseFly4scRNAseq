{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import os\n",
    "from collections import OrderedDict as odict\n",
    "from functools import reduce\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix,cohen_kappa_score\n",
    "from bokeh.plotting import figure,output_file,output_notebook,show\n",
    "import bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SimHash\n",
    "class LSH(object):\n",
    "    def __init__(self,data,label,hash_length,nnn):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: the projection numbers\n",
    "        nnn: n nearest Neighbors\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True)\n",
    "        self.label=label\n",
    "        self.weights=np.random.random((data.shape[1],hash_length))\n",
    "        self.hashes=(self.data@self.weights)>0\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.nnn=nnn\n",
    "        self.create_bins()\n",
    "        \n",
    "    def query(self,query_data,nnn,not_olap=False):\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_hash=(query_data@self.weights)>0\n",
    "        L1_distances=np.sum(np.abs(query_hash^self.hashes),axis=1)\n",
    "        nnn=min(self.hashes.shape[0],nnn)\n",
    "        if not_olap:\n",
    "            no_overlaps=np.sum(L1_distances==self.maxl1distance)\n",
    "            return no_overlaps\n",
    "\n",
    "        NNs=L1_distances.argsort()\n",
    "        NNs=NNs[:nnn]\n",
    "        return NNs\n",
    "\n",
    "    def create_bins(self):\n",
    "        if hasattr(self,'bins'):\n",
    "            return\n",
    "        start=time.time()\n",
    "        self.bins=np.unique(self.hashes,axis=0)\n",
    "        self.num_bins=self.bins.shape[0]\n",
    "        assignment=np.zeros(self.hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.bins):\n",
    "            assignment[(self.hashes==_bin).all(axis=1)]=idx\n",
    "        self.binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.bins.shape[0])}\n",
    "        self.pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "        self.timetoindex=time.time()-start\n",
    "\n",
    "    def query_bins(self,query_data,search_radius=1,order=True):\n",
    "        if not hasattr(self,'bins'):\n",
    "            raise ValueError('Bins for model not created')\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_hash = (query_data@self.weights)>0\n",
    "        valid_bins=np.flatnonzero((query_hash[None,:]^self.bins).sum(axis=1)<=search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(query_hash^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        return all_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#FlyHash\n",
    "class flylsh(LSH):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        \"\"\"\n",
    "        data: Nxd matrix\n",
    "        label: N*1 vector\n",
    "        hash_length: scalar\n",
    "        nnn: n nearest neighbors\n",
    "        sampling_ratio: fraction of input dims to sample from when producing a hash\n",
    "        embedding_size: dimensionality of projection space, m\n",
    "        Note that in Flylsh, the hash length and embedding_size are NOT the same\n",
    "        whereas in usual LSH they are\n",
    "        \"\"\"\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True)\n",
    "        self.label=label\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        num_projections=int(sampling_ratio*data.shape[1])\n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        yindices=np.arange(weights.shape[1])[None,:]\n",
    "        xindices=weights.argsort(axis=0)[-num_projections:,:]\n",
    "        self.weights=np.zeros_like(weights,dtype=np.bool)\n",
    "        self.weights[xindices,yindices]= True#sparse projection vectors\n",
    "        \n",
    "        all_activations=(self.data@self.weights)\n",
    "        xindices=np.arange(data.shape[0])[:,None]\n",
    "        yindices=all_activations.argsort(axis=1)[:,-hash_length:]\n",
    "        self.hashes=np.zeros_like(all_activations,dtype=np.bool)\n",
    "        self.hashes[xindices,yindices]=True #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        \n",
    "        self.create_bins()\n",
    "        self.create_lowd_bins()\n",
    "\n",
    "    def create_lowd_bins(self):\n",
    "        start=time.time()\n",
    "        self.lowd_bins=np.unique(self.lowd_hashes,axis=0)\n",
    "        #self.num_bins=self.bins.shape[0]\n",
    "\n",
    "        assignment=np.zeros(self.lowd_hashes.shape[0])\n",
    "        for idx,_bin in enumerate(self.lowd_bins):\n",
    "            assignment[(self.lowd_hashes==_bin).all(axis=1)]=idx\n",
    "        self.lowd_binstopoints={bin_idx:np.flatnonzero(assignment==bin_idx) for bin_idx in range(self.lowd_bins.shape[0])}\n",
    "        self.lowd_pointstobins={point:int(_bin) for point,_bin in enumerate(assignment)}\n",
    "        self.timetoindex=time.time()-start\n",
    "\n",
    "    def query_lowd_bins(self,query_data,search_radius=1,order=True):\n",
    "        if not hasattr(self,'lowd_bins'):\n",
    "            raise ValueError('low dimensional bins for model not created')\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_activation=(query_data@self.weights)\n",
    "        indices=query_activation.argsort()[-hash_length:]\n",
    "        query_hashes=np.zeros_like(qh,dtype=np.bool)\n",
    "        query_hashes[indices]=True\n",
    "        query_bin=self.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.lowd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        return all_points\n",
    "    \n",
    "    def query(self,query_data,nnn,not_olap=False):\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_activation=(query_data@self.weights)\n",
    "        indices=query_activation.argsort()[-self.hash_length:]\n",
    "        query_hash=np.zeros_like(query_activation,dtype=np.bool)\n",
    "        query_hash[indices]=True\n",
    "        \n",
    "        L1_distances=np.sum(np.abs(query_hash^self.hashes),axis=1)\n",
    "        nnn=min(self.hashes.shape[0],nnn)\n",
    "        if not_olap:\n",
    "            no_overlaps=np.sum(L1_distances==self.maxl1distance)\n",
    "            return no_overlaps\n",
    "\n",
    "        NNs=L1_distances.argsort()\n",
    "        NNs=NNs[:nnn]\n",
    "        return NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DenseFly\n",
    "class denseflylsh(flylsh):\n",
    "    def __init__(self,data,label,hash_length,nnn,sampling_ratio,embedding_size):\n",
    "        self.hash_length=hash_length\n",
    "        self.embedding_size=embedding_size\n",
    "        K=embedding_size//hash_length\n",
    "        self.data=data-np.mean(data,axis=1,keepdims=True)\n",
    "        self.label=label\n",
    "        self.group_counts=np.unique(self.label).shape[0]\n",
    "        self.nnn=nnn\n",
    "        \n",
    "        weights=np.random.random((data.shape[1],embedding_size))\n",
    "        self.weights=(weights>1-sampling_ratio) #sparse projection vectors\n",
    "        all_activations=(self.data@self.weights)\n",
    "        threshold=0\n",
    "        self.hashes=(all_activations>=threshold) #choose topk activations\n",
    "        self.dense_activations=all_activations\n",
    "        self.sparse_activations=self.hashes.astype(np.float32)*all_activations #elementwise product\n",
    "        self.maxl1distance=2*self.hash_length\n",
    "        self.lowd_hashes=all_activations.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        \n",
    "        self.create_lowd_bins()\n",
    "        \n",
    "        \n",
    "    def query_lowd_bins(self,query_data,search_radius=1,order=True):\n",
    "        if not hasattr(self,'lowd_bins'):\n",
    "            raise ValueError('low dimensional bins for model not created')\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_activation=(query_data@self.weights)\n",
    "        query_hashes=query_activation>0\n",
    "        query_bin=self.reshape((-1,hash_length,K)).sum(axis=-1) > 0\n",
    "        valid_bins=np.flatnonzero((query_bin[None,:]^self.lowd_bins).sum(axis=1)<=search_radius)\n",
    "        all_points=reduce(np.union1d,np.array([self.lowd_binstopoints[idx] for idx in valid_bins]))\n",
    "        if order:\n",
    "            l1distances=(self.hashes[qidx,:]^self.hashes[all_points,:]).sum(axis=1)\n",
    "            all_points=all_points[l1distances.argsort()]\n",
    "        return all_points\n",
    "    \n",
    "    def query(self,query_data,nnn,not_olap=False):\n",
    "        query_data=query_data-np.mean(query_data)\n",
    "        query_activation=(query_data@self.weights)\n",
    "        query_hash=query_activation>0\n",
    "        \n",
    "        L1_distances=np.sum(np.abs(query_hash^self.hashes),axis=1)\n",
    "        nnn=min(self.hashes.shape[0],nnn)\n",
    "        if not_olap:\n",
    "            no_overlaps=np.sum(L1_distances==self.maxl1distance)\n",
    "            return no_overlaps\n",
    "\n",
    "        NNs=L1_distances.argsort()\n",
    "        NNs=NNs[:nnn]\n",
    "        return NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_CKS(matrix):\n",
    "        po = matrix.trace()/np.sum(matrix)\n",
    "        pe = sum(np.sum(matrix,axis=0)*np.sum(matrix,axis=1))/np.sum(matrix)/np.sum(matrix)\n",
    "        return (po-pe)/(1-pe)\n",
    "    \n",
    "def test_batcheffect(model_1,model_2):\n",
    "    # query cells from batch 1 with batch 2 data as the reference\n",
    "    cm = 0\n",
    "    for i in range(5):\n",
    "        query_indices = np.random.choice(model_1.data.shape[0],model_1.data.shape[0]//5)\n",
    "        for index in query_indices:\n",
    "            NNs = model_2.query(model_1.data[index],model_2.nnn)\n",
    "            pre_label= model_2.label[NNs]\n",
    "            true_label = np.array([model_1.label[index]]*pre_label.shape[0])\n",
    "            cm += confusion_matrix(true_label,pre_label,np.unique(model_2.label))\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data and divide matrix into two batches and their labels\n",
    "path = \"../data/Batch_Data/Batch_Data.txt\"    \n",
    "data=pd.read_table(path,sep=' ')\n",
    "index_b1=data[\"Batch\"]==\"Batch1\"\n",
    "index_b2=data[\"Batch\"]==\"Batch2\"\n",
    "data_1 = np.array(data[index_b1].iloc[:,:-2])\n",
    "label_1 = np.array(data[index_b1].iloc[:,-1])\n",
    "data_2 = np.array(data[index_b2].iloc[:,:-2])\n",
    "label_2 = np.array(data[index_b2].iloc[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting parameters\n",
    "hash_length=64 # hash length can be selected from [64,128,256,512,1024]\n",
    "nnn=10\n",
    "#construct DenseFly models\n",
    "print('DenseFly')\n",
    "densemodel_1=denseflylsh(data_1,label_1,hash_length,nnn,0.1,20*hash_length)\n",
    "densemodel_2=denseflylsh(data_2,label_2,hash_length,nnn,0.1,20*hash_length)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(densemodel_1,densemodel_2)\n",
    "cm_2to1 = test_batcheffect(densemodel_2,densemodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)\n",
    "\n",
    "#construct FlyHash models\n",
    "print('FlyHash')\n",
    "flymodel_1=flylsh(data_1,label_1,hash_length,nnn,0.1,20*hash_length)\n",
    "flymodel_2=flylsh(data_2,label_2,hash_length,nnn,0.1,20*hash_length)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(flymodel_1,flymodel_2)\n",
    "cm_2to1 = test_batcheffect(flymodel_2,flymodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)\n",
    "\n",
    "#construct SimHash models\n",
    "print('SimHash')\n",
    "lshmodel_1=LSH(data_1,label_1,hash_length,nnn)\n",
    "lshmodel_2=LSH(data_2,label_2,hash_length,nnn)\n",
    "#get confusion_matrix and compute cohen kappa score\n",
    "cm_1to2 = test_batcheffect(lshmodel_1,lshmodel_2)\n",
    "cm_2to1 = test_batcheffect(lshmodel_2,lshmodel_1)\n",
    "compute_CKS(cm_1to2),compute_CKS(cm_2to1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (densefly)",
   "language": "python",
   "name": "densefly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
